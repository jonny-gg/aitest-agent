"""AIæµ‹è¯•ç”ŸæˆAgent"""
import os
import asyncio
from pathlib import Path
from typing import Dict, List
from datetime import datetime
from loguru import logger
from uuid import uuid4

from app.services.git_service import GitService
from app.services.code_analyzer import get_analyzer
from app.services.test_generator import get_test_generator
from app.services.test_executor import get_test_executor


class TestGenerationAgent:
    """æµ‹è¯•ç”ŸæˆAgent - ç¼–æ’æ•´ä¸ªæµ‹è¯•ç”Ÿæˆæµç¨‹"""
    
    def __init__(self):
        self.git_service = GitService()
    
    async def execute(
        self,
        project_id: str,
        project_config: Dict,
        task_id: str,
        progress_callback=None
    ) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•ç”Ÿæˆæµç¨‹
        
        Args:
            project_id: é¡¹ç›®ID
            project_config: é¡¹ç›®é…ç½®
            task_id: ä»»åŠ¡ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæµ‹è¯•ç”Ÿæˆä»»åŠ¡: {task_id}")
        
        result = {
            'success': False,
            'commit_hash': None,
            'test_files': [],
            'test_results': {},
            'coverage': {},
            'error': None
        }
        
        try:
            # 1. å…‹éš†/æ›´æ–°ä»£ç ä»“åº“
            await self._update_progress(progress_callback, 10, "CLONING", "å…‹éš†ä»£ç ä»“åº“...")
            repo_path = await self.git_service.clone_or_pull(
                project_id,
                project_config['git_url'],
                project_config['git_branch']
            )
            
            commit_info = await self.git_service.get_commit_info(repo_path)
            result['commit_hash'] = commit_info['hash']
            
            logger.info(f"ğŸ“ ä»£ç ä»“åº“: {repo_path}")
            
            # 2. åˆ†æä»£ç 
            await self._update_progress(progress_callback, 30, "ANALYZING", "åˆ†æä»£ç ç»“æ„...")
            analyzer = get_analyzer(project_config['language'])
            
            source_dir = Path(repo_path) / project_config.get('source_directory', '.')
            analysis_results = analyzer.analyze_directory(str(source_dir))
            
            logger.info(f"ğŸ” å‘ç° {len(analysis_results)} ä¸ªæ–‡ä»¶å¾…æµ‹è¯•")
            
            # 3. ç”Ÿæˆæµ‹è¯•ï¼ˆæ™ºèƒ½è·³è¿‡å·²æœ‰æµ‹è¯•ï¼‰
            await self._update_progress(progress_callback, 50, "GENERATING", "æ£€æŸ¥å¹¶ç”Ÿæˆæµ‹è¯•ä»£ç ...")
            test_generator = get_test_generator(
                project_config['language'],
                project_config.get('ai_provider', 'openai'),
                repo_path
            )
            
            generated_tests = []
            existing_tests = []
            test_dir = Path(repo_path) / project_config.get('test_directory', 'tests')
            test_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¸ºæ¯ä¸ªæºæ–‡ä»¶ç”Ÿæˆæµ‹è¯•ï¼ˆä¸€ä¸ªæºæ–‡ä»¶ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰å‡½æ•°çš„æµ‹è¯•ï¼‰
            test_metadata = {}  # å­˜å‚¨æµ‹è¯•å…ƒæ•°æ®ï¼Œç”¨äºåç»­ä¿®å¤
            max_concurrent = project_config.get('max_concurrent_generations', 10)  # æœ€å¤§å¹¶å‘æ•°
            
            # æŒ‰æºæ–‡ä»¶åˆ†ç»„å‡†å¤‡æµ‹è¯•ä»»åŠ¡
            test_tasks = []  # æ¯ä¸ªä»»åŠ¡å¯¹åº”ä¸€ä¸ªæºæ–‡ä»¶
            skipped_count = 0
            skip_existing = project_config.get('skip_existing_tests', True)
            
            for file_analysis in analysis_results:
                # æ£€æŸ¥è¯¥æºæ–‡ä»¶çš„æµ‹è¯•æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                expected_test_file = self._get_expected_test_file_path(
                    test_dir,
                    file_analysis['file_path'],
                    project_config['language'],
                    project_config.get('test_framework')
                )
                
                if skip_existing and expected_test_file.exists():
                    # æµ‹è¯•æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ•´ä¸ªæ–‡ä»¶
                    if str(expected_test_file) not in existing_tests:
                        existing_tests.append(str(expected_test_file))
                    test_metadata[str(expected_test_file)] = {
                        'file_analysis': file_analysis,
                        'is_existing': True
                    }
                    skipped_count += 1
                    logger.info(f"â­ï¸  è·³è¿‡å·²æœ‰æµ‹è¯•: {expected_test_file.name}")
                else:
                    # éœ€è¦ä¸ºè¯¥æºæ–‡ä»¶ç”Ÿæˆæ–°æµ‹è¯•
                    test_tasks.append({
                        'file_analysis': file_analysis
                    })
            
            logger.info(f"ğŸ“Š è·³è¿‡ {skipped_count} ä¸ªå·²æœ‰æµ‹è¯•ï¼Œå‡†å¤‡å¹¶å‘ç”Ÿæˆ {len(test_tasks)} ä¸ªæ–°æµ‹è¯•ï¼ˆå¹¶å‘æ•°ï¼š{max_concurrent}ï¼‰")
            
            # å¹¶å‘ç”Ÿæˆæ–°æµ‹è¯•
            if test_tasks:
                total_files = len(test_tasks)
                
                # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹ç”Ÿæˆ
                await self._update_progress(
                    progress_callback, 
                    50, 
                    "GENERATING", 
                    f"å¼€å§‹ç”Ÿæˆæµ‹è¯•ä»£ç : 0/{total_files} ä¸ªæ–‡ä»¶"
                )
                
                generated_results = await self._generate_tests_concurrently(
                    test_tasks,
                    test_generator,
                    test_dir,
                    project_config,
                    max_concurrent,
                    progress_callback  # ä¼ é€’è¿›åº¦å›è°ƒ
                )
                
                # å¤„ç†ç”Ÿæˆç»“æœ
                generated_count = 0
                for result_item in generated_results:
                    if result_item['success']:
                        test_file = result_item['test_file']
                        generated_tests.append(test_file)
                        test_metadata[test_file] = {
                            'file_analysis': result_item['file_analysis'],
                            'test_code': result_item['test_code'],
                            'is_existing': False
                        }
                        generated_count += 1
                        
                        # æ›´æ–°è¯¦ç»†è¿›åº¦
                        progress_percent = 50 + int((generated_count / total_files) * 15)  # 50-65%
                        await self._update_progress(
                            progress_callback,
                            progress_percent,
                            "GENERATING",
                            f"ç”Ÿæˆæµ‹è¯•ä»£ç : {generated_count}/{total_files} ä¸ªæ–‡ä»¶"
                        )
                        
                        logger.info(f"âœ… ç”Ÿæˆæµ‹è¯• ({generated_count}/{total_files}): {Path(test_file).name}")
                    else:
                        source_file = Path(result_item['file_analysis']['file_path']).name
                        logger.warning(f"âš ï¸  ä¸ºæºæ–‡ä»¶ {source_file} ç”Ÿæˆæµ‹è¯•å¤±è´¥: {result_item['error']}")
            
            # æ‰€æœ‰æµ‹è¯•æ–‡ä»¶ï¼ˆæ–°ç”Ÿæˆçš„ + å·²å­˜åœ¨çš„ï¼‰
            all_test_files = generated_tests + existing_tests
            result['test_files'] = all_test_files
            logger.info(f"ğŸ“ æ–°ç”Ÿæˆ {len(generated_tests)} ä¸ªæµ‹è¯•ï¼Œå·²æœ‰ {len(existing_tests)} ä¸ªæµ‹è¯•ï¼Œå…± {len(all_test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
            
            # 4. æ‰§è¡Œæµ‹è¯•å¹¶è‡ªåŠ¨ä¿®å¤å¤±è´¥çš„æµ‹è¯•
            await self._update_progress(progress_callback, 68, "TESTING", "æ‰§è¡Œæµ‹è¯•...")
            test_executor = get_test_executor(
                project_config['language'], 
                repo_path,
                project_config.get('test_framework')
            )
            
            # é¦–æ¬¡æ‰§è¡Œæµ‹è¯•
            test_results = test_executor.execute_tests(generated_tests)
            result['test_results'] = test_results
            
            logger.info(f"ğŸ§ª æµ‹è¯•ç»“æœ: {test_results['passed_count']}/{test_results['total']} é€šè¿‡")
            
            # 5. å¦‚æœæµ‹è¯•å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            enable_auto_fix = project_config.get('enable_auto_fix', True)
            max_retry = project_config.get('max_test_fix_retries', 3)
            
            if enable_auto_fix and not test_results['passed'] and test_results['failed_count'] > 0:
                logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {test_results['failed_count']} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œå¼€å§‹è‡ªåŠ¨ä¿®å¤...")
                
                fixed_tests = await self._fix_failed_tests(
                    test_executor,
                    test_generator,
                    generated_tests,
                    test_metadata,
                    test_results,
                    project_config,
                    max_retry,
                    progress_callback
                )
                
                if fixed_tests > 0:
                    logger.info(f"âœ… æˆåŠŸä¿®å¤ {fixed_tests} ä¸ªæµ‹è¯•")
                    # é‡æ–°æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
                    test_results = test_executor.execute_tests(generated_tests)
                    result['test_results'] = test_results
                    logger.info(f"ğŸ§ª ä¿®å¤åæµ‹è¯•ç»“æœ: {test_results['passed_count']}/{test_results['total']} é€šè¿‡")
            elif not enable_auto_fix and test_results['failed_count'] > 0:
                logger.info(f"âš ï¸  è‡ªåŠ¨ä¿®å¤åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æµ‹è¯•ä¿®å¤")
            
            # 6. æ”¶é›†è¦†ç›–ç‡
            await self._update_progress(progress_callback, 85, "COLLECTING_COVERAGE", "æ”¶é›†è¦†ç›–ç‡...")
            
            if test_results.get('coverage_file'):
                coverage_data = test_executor.collect_coverage(test_results['coverage_file'])
                result['coverage'] = coverage_data
                logger.info(f"ğŸ“Š ä»£ç è¦†ç›–ç‡: {coverage_data.get('line_coverage', 0)}%")
            
            # 7. æäº¤ä»£ç 
            if project_config.get('auto_commit', True):
                await self._update_progress(progress_callback, 95, "COMMITTING", "æäº¤æµ‹è¯•ä»£ç ...")
                
                branch_name = f"aitest/add-tests-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
                commit_message = f"""Add unit tests generated by AI Test Agent

Generated {len(generated_tests)} test files
Coverage: {result['coverage'].get('line_coverage', 0):.2f}%

Task ID: {task_id}
"""
                
                try:
                    commit_hash = await self.git_service.commit_and_push(
                        repo_path,
                        generated_tests,
                        commit_message,
                        branch_name
                    )
                    
                    logger.info(f"âœ… ä»£ç å·²æäº¤: {commit_hash[:8]}")
                    
                    # åˆ›å»ºPRï¼ˆå¦‚æœé…ç½®ï¼‰
                    if project_config.get('create_pr', True):
                        await self.git_service.create_pull_request(
                            project_id,
                            branch_name,
                            "Add AI-generated unit tests",
                            commit_message,
                            project_config['git_branch']
                        )
                
                except Exception as e:
                    logger.warning(f"æäº¤ä»£ç å¤±è´¥: {e}")
                    # æäº¤å¤±è´¥ä¸å½±å“æ•´ä½“æˆåŠŸ
            
            # 8. å®Œæˆ
            await self._update_progress(progress_callback, 100, "COMPLETED", "ä»»åŠ¡å®Œæˆ!")
            result['success'] = True
            
            logger.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆ: {task_id}")
            return result
        
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            result['error'] = str(e)
            await self._update_progress(progress_callback, 0, "FAILED", f"å¤±è´¥: {e}")
            return result
    
    async def _generate_tests_concurrently(
        self,
        test_tasks: List[Dict],
        test_generator,
        test_dir: Path,
        project_config: Dict,
        max_concurrent: int,
        progress_callback=None
    ) -> List[Dict]:
        """
        å¹¶å‘ç”Ÿæˆå¤šä¸ªæµ‹è¯•ï¼ˆæ¯ä¸ªä»»åŠ¡å¯¹åº”ä¸€ä¸ªæºæ–‡ä»¶ï¼‰
        
        Args:
            test_tasks: æµ‹è¯•ä»»åŠ¡åˆ—è¡¨ï¼ˆæ¯ä¸ªä»»åŠ¡åŒ…å«ä¸€ä¸ªæºæ–‡ä»¶çš„æ‰€æœ‰å‡½æ•°ï¼‰
            test_generator: æµ‹è¯•ç”Ÿæˆå™¨
            test_dir: æµ‹è¯•ç›®å½•
            project_config: é¡¹ç›®é…ç½®
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def generate_test_for_file(task):
            """ä¸ºä¸€ä¸ªæºæ–‡ä»¶çš„æ‰€æœ‰å‡½æ•°ç”Ÿæˆæµ‹è¯•ï¼ˆå«è¯­æ³•éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤ï¼‰"""
            async with semaphore:
                try:
                    file_analysis = task['file_analysis']
                    source_file = Path(file_analysis['file_path']).name
                    
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„AIè°ƒç”¨ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                    loop = asyncio.get_event_loop()
                    
                    # ç”Ÿæˆæµ‹è¯•ä»£ç å¹¶è‡ªåŠ¨éªŒè¯ä¿®å¤
                    def generate_and_validate_with_hybrid():
                        return test_generator.generate_and_validate(
                            file_analysis,
                            project_config['language'],
                            project_config['test_framework'],
                            test_dir=test_dir,
                            use_hybrid_mode=True,
                            max_fix_attempts=3  # æœ€å¤šå°è¯•ä¿®å¤3æ¬¡
                        )
                    
                    result = await loop.run_in_executor(None, generate_and_validate_with_hybrid)
                    
                    # æ£€æŸ¥ç”Ÿæˆå’ŒéªŒè¯æ˜¯å¦æˆåŠŸ
                    if not result['success']:
                        error_msg = f"è¯­æ³•éªŒè¯å¤±è´¥ (å°è¯• {result['attempts']} æ¬¡): {result['validation_errors']}"
                        logger.error(f"âŒ {source_file}: {error_msg}")
                        return {
                            'success': False,
                            'file_analysis': file_analysis,
                            'error': error_msg,
                            'validation_errors': result['validation_errors']
                        }
                    
                    test_code = result['test_code']
                    attempts = result['attempts']
                    
                    if attempts > 1:
                        logger.info(f"âœ… {source_file}: ç»è¿‡ {attempts} æ¬¡ä¿®å¤åé€šè¿‡è¯­æ³•éªŒè¯")
                    else:
                        logger.info(f"âœ… {source_file}: é¦–æ¬¡ç”Ÿæˆå³é€šè¿‡è¯­æ³•éªŒè¯")
                    
                    # ä¿å­˜æµ‹è¯•æ–‡ä»¶
                    test_file = self._save_test_file(
                        test_dir,
                        file_analysis['file_path'],
                        test_code,
                        project_config['language'],
                        project_config.get('test_framework')
                    )
                    
                    return {
                        'success': True,
                        'test_file': test_file,
                        'file_analysis': file_analysis,
                        'test_code': test_code,
                        'validation_attempts': attempts
                    }
                    
                except Exception as e:
                    logger.error(f"âŒ ç”Ÿæˆæµ‹è¯•å¼‚å¸¸: {str(e)}")
                    return {
                        'success': False,
                        'file_analysis': task['file_analysis'],
                        'error': str(e)
                    }
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*[generate_test_for_file(task) for task in test_tasks])
        return results
    
    def _get_expected_test_file_path(
        self,
        test_dir: Path,
        source_file: str,
        language: str,
        test_framework: str = None
    ) -> Path:
        """è·å–é¢„æœŸçš„æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆä¸€ä¸ªæºæ–‡ä»¶å¯¹åº”ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼‰"""
        # æ ¹æ®è¯­è¨€ç¡®å®šæ–‡ä»¶æ‰©å±•å
        extensions = {
            'golang': '_test.go',
            'cpp': '_test.cpp',
            'c': '_test.c'
        }
        
        # ç”Ÿæˆæµ‹è¯•æ–‡ä»¶åï¼šæºæ–‡ä»¶å + æµ‹è¯•åç¼€
        source_name = Path(source_file).stem
        test_file_name = f"{source_name}{extensions.get(language, '_test.txt')}"
        
        return test_dir / test_file_name
    
    def _save_test_file(
        self,
        test_dir: Path,
        source_file: str,
        test_code: str,
        language: str,
        test_framework: str = None
    ) -> str:
        """ä¿å­˜æµ‹è¯•æ–‡ä»¶å¹¶è‡ªåŠ¨ä¿®å¤"""
        test_file_path = self._get_expected_test_file_path(
            test_dir,
            source_file,
            language,
            test_framework
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤§å°å†™å†²çªçš„æ–‡ä»¶
        if test_file_path.exists():
            # æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å¤§å°å†™ä¸åŒçš„é‡å¤
            existing_files = list(test_file_path.parent.glob(f"{test_file_path.stem}*{test_file_path.suffix}"))
            for existing_file in existing_files:
                if existing_file.name.lower() == test_file_path.name.lower() and existing_file != test_file_path:
                    logger.warning(f"âš ï¸  æ£€æµ‹åˆ°å¤§å°å†™å†²çªçš„æ–‡ä»¶: {existing_file.name} vs {test_file_path.name}")
                    # ä¿ç•™åŸæ–‡ä»¶ï¼Œè·³è¿‡æ–°æ–‡ä»¶
                    return str(existing_file)
        
        # ä¿®å¤åŒ…åï¼ˆå¯¹äº Golangï¼‰
        if language == 'golang':
            test_code = self._fix_package_name(test_code, test_dir)
        
        # å†™å…¥æµ‹è¯•ä»£ç 
        with open(test_file_path, 'w', encoding='utf-8') as f:
            f.write(test_code)
        
        logger.debug(f"âœ… ä¿å­˜æµ‹è¯•æ–‡ä»¶: {test_file_path.name}")
        return str(test_file_path)
    
    def _fix_package_name(self, test_code: str, test_dir: Path) -> str:
        """
        ä¿®å¤æµ‹è¯•æ–‡ä»¶çš„åŒ…å
        
        ä½¿ç”¨åŒåŒ…æµ‹è¯•ç­–ç•¥ï¼šæµ‹è¯•ä»£ç å’Œæºä»£ç ä½¿ç”¨ç›¸åŒçš„ package
        ä¸ä½¿ç”¨ _test åç¼€
        """
        import re
        
        # è·å–ç›®å½•åä½œä¸ºåŒ…åï¼ˆä¸æ·»åŠ  _test åç¼€ï¼‰
        dir_name = test_dir.name
        correct_package = dir_name  # âœ… ä½¿ç”¨åŒåŒ…æµ‹è¯•ï¼Œä¸æ·»åŠ  _test
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢åŒ…åï¼ˆç§»é™¤ _test åç¼€ï¼‰
        package_match = re.search(r'^package\s+(\w+)', test_code, re.MULTILINE)
        if package_match:
            current_package = package_match.group(1)
            # å¦‚æœå½“å‰åŒ…åæœ‰ _test åç¼€ï¼Œç§»é™¤å®ƒ
            if current_package.endswith('_test'):
                test_code = re.sub(
                    r'^package\s+\w+_test',
                    f'package {correct_package}',
                    test_code,
                    count=1,
                    flags=re.MULTILINE
                )
                logger.debug(f"ğŸ“ ä¿®å¤åŒ…åä¸ºåŒåŒ…æµ‹è¯•: {current_package} -> {correct_package}")
            elif current_package != correct_package:
                # å¦‚æœåŒ…åä¸åŒ¹é…ï¼ˆä½†æ²¡æœ‰_teståç¼€ï¼‰ï¼Œä¹Ÿä¿®æ­£
                test_code = re.sub(
                    r'^package\s+\w+',
                    f'package {correct_package}',
                    test_code,
                    count=1,
                    flags=re.MULTILINE
                )
                logger.debug(f"ğŸ“ ä¿®å¤åŒ…å: {current_package} -> {correct_package}")
        
        return test_code
    
    async def _fix_failed_tests(
        self,
        test_executor,
        test_generator,
        test_files: List[str],
        test_metadata: Dict,
        test_results: Dict,
        project_config: Dict,
        max_retry: int,
        progress_callback
    ) -> int:
        """
        è‡ªåŠ¨ä¿®å¤å¤±è´¥çš„æµ‹è¯•
        
        Args:
            test_executor: æµ‹è¯•æ‰§è¡Œå™¨
            test_generator: æµ‹è¯•ç”Ÿæˆå™¨
            test_files: æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
            test_metadata: æµ‹è¯•å…ƒæ•°æ®
            test_results: æµ‹è¯•ç»“æœ
            project_config: é¡¹ç›®é…ç½®
            max_retry: æœ€å¤§é‡è¯•æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            æˆåŠŸä¿®å¤çš„æµ‹è¯•æ•°é‡
        """
        fixed_count = 0
        retry_count = 0
        
        while retry_count < max_retry and test_results.get('failed_count', 0) > 0:
            retry_count += 1
            logger.info(f"ğŸ”„ ç¬¬ {retry_count}/{max_retry} æ¬¡ä¿®å¤å°è¯•")
            
            await self._update_progress(
                progress_callback,
                70 + retry_count * 3,
                "FIXING",
                f"ä¿®å¤å¤±è´¥çš„æµ‹è¯• (ç¬¬{retry_count}æ¬¡å°è¯•)..."
            )
            
            # é€ä¸ªæµ‹è¯•æ–‡ä»¶æ‰§è¡Œï¼Œæ‰¾å‡ºå¤±è´¥çš„æµ‹è¯•
            for test_file in test_files:
                try:
                    # å•ç‹¬æ‰§è¡Œè¿™ä¸ªæµ‹è¯•æ–‡ä»¶
                    single_result = test_executor.execute_tests([test_file])
                    
                    # å¦‚æœè¿™ä¸ªæµ‹è¯•å¤±è´¥äº†
                    if not single_result['passed'] and single_result['failed_count'] > 0:
                        logger.info(f"ğŸ” åˆ†æå¤±è´¥æµ‹è¯•: {test_file}")
                        
                        # è·å–æµ‹è¯•å…ƒæ•°æ®
                        if test_file not in test_metadata:
                            logger.warning(f"âš ï¸  æ‰¾ä¸åˆ°æµ‹è¯•å…ƒæ•°æ®: {test_file}")
                            continue
                        
                        metadata = test_metadata[test_file]
                        
                        # è¯»å–å½“å‰æµ‹è¯•ä»£ç 
                        with open(test_file, 'r', encoding='utf-8') as f:
                            current_test_code = f.read()
                        
                        # ä½¿ç”¨AIä¿®å¤æµ‹è¯•
                        try:
                            fixed_test_code = test_generator.fix_test(
                                current_test_code,
                                single_result['output'],
                                metadata['file_analysis'],
                                project_config['language'],
                                project_config['test_framework']
                            )
                            
                            # ä¿å­˜ä¿®å¤åçš„æµ‹è¯•
                            with open(test_file, 'w', encoding='utf-8') as f:
                                f.write(fixed_test_code)
                            
                            # æ›´æ–°å…ƒæ•°æ®
                            metadata['test_code'] = fixed_test_code
                            
                            # éªŒè¯ä¿®å¤åçš„æµ‹è¯•
                            verify_result = test_executor.execute_tests([test_file])
                            if verify_result['passed']:
                                logger.info(f"âœ… æµ‹è¯•ä¿®å¤æˆåŠŸ: {test_file}")
                                fixed_count += 1
                            else:
                                logger.warning(f"âš ï¸  æµ‹è¯•ä¿®å¤åä»ç„¶å¤±è´¥: {test_file}")
                        
                        except Exception as e:
                            logger.error(f"âŒ ä¿®å¤æµ‹è¯•å¤±è´¥: {test_file}, é”™è¯¯: {e}")
                            continue
                
                except Exception as e:
                    logger.error(f"âŒ æ‰§è¡Œæµ‹è¯•å¤±è´¥: {test_file}, é”™è¯¯: {e}")
                    continue
            
            # é‡æ–°æ‰§è¡Œæ‰€æœ‰æµ‹è¯•æŸ¥çœ‹æ•´ä½“æƒ…å†µ
            test_results = test_executor.execute_tests(test_files)
            logger.info(f"ğŸ“Š å½“å‰æµ‹è¯•çŠ¶æ€: {test_results['passed_count']}/{test_results['total']} é€šè¿‡")
            
            # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼Œæå‰é€€å‡º
            if test_results['passed']:
                logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
                break
        
        return fixed_count
    
    async def _update_progress(
        self,
        callback,
        progress: int,
        status: str,
        message: str
    ):
        """æ›´æ–°è¿›åº¦"""
        logger.info(f"[{progress}%] {status}: {message}")
        
        if callback:
            await callback(progress, status, message)

